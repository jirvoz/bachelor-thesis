\chapter{Introduction}
Performance of operating system is crucial, because it can significantly affect all the applications running above it.
When a regression in new version occurs, business applications can be even financially affected.

Scheduler was simple. Multi-core CPUs made the scheduler little more complex, but those complications were tuned through time.
Next milestone was introduction of multi-CPU machines with separated memory for each physical unit.

Compared to functional testing, performance isn't evaluated as true/false result, but relative change to previous measurement.
Due to this complexity is hard to use common tools for inspecting performance.
The biggest regression in scheduler doesn't hide in uneffective code, but in wrong placement of processes to cores and it's queues.

The usual testing method is simulating load simmilar to real usage.
To achieve this there are many benchmarks targetting different types of load, usually many parallel process, sometimes communicating between each other.

Good performance tests usually create great amount of data, which is required to be processed, aggregaed and visualised.

\chapter{Linux process scheduling}

\chapter{Performance measurement}

\chapter{Displaying the results}

\chapter{Notes}
\begin{verbatim}
Linux scheduler
    CFS
    numa planning
        not easy for scheduler
        manual pinning option
        group imbalance bug
    tune profiles
        focus on throughput or latency
Performance
    performance isn't simple pass/fail
    comparison of baseline and target kernel
    provisioning of machines with Beaker
    collecting system load data
        mpstat
            usage of every cpu
        numatop
            usage of memory
        ps with psr column
            cpu time
        time
            total, user and system time spent
        turbostat
        lstopo for hardware schema image
    benchmarks
        specjbb2005
        specjvm2008
        nas parallel
        linpack
        stream (memory throughput)
        (specjbb2015 - not stable)
    result storing
        preprocessing to xml
        storing to database
    plotting
        lstopo for plotting hardware topology
        bargraphs to show inaccuaracy
            operations per second (runtime of benchmark)
            time out of cpu
        detailed comparsion of two results
        timeline with many results over longer period of time
        heatmaps for core usage over time
            http://www.brendangregg.com/HeatMaps/utilization.html
\end{verbatim}

\chapter{Specification}
\begin{enumerate}
\item Get acquainted with the existing methods for measuring performance of the Linux kernel scheduler and with means of storing of benchmarks results for further processing.
\item Study possible ways of processing these results with a focus on graphic interpretation and on methods for detection of performance degradation.
\item Design and implement a method for efficient graphic interpretation of long-term measurements.
\item Design and implement a method for automatic detection of performance regression.
\item Demonstrate the functionality of your implementation on at least two versions of the Linux kernel.
\item Evaluate the obtained results and discuss possibilities of further development of the project, especially of the automatic detection of performance regression.
\end{enumerate}

Literature:
\begin{itemize}
\item Lozi, Jean-Pierre, et al. "The Linux scheduler: a decade of wasted cores." Proceedings of the Eleventh European Conference on Computer Systems. ACM, 2016.
\item Daniel, P., and Cesati Marco. "Understanding the Linux kernel." (2007).
\item Bailey, David H., et al. "The NAS parallel benchmarks." The International Journal of Supercomputing Applications 5.3 (1991): 63-73.
\end{itemize}
