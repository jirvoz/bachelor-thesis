\chapter{Introduction}
Performance of operating system is crucial, because it can significantly affect all the applications running above it.
When a regression in new version occurs, business applications can be even financially affected.

Scheduler was simple. Multi-core CPUs made the scheduler little more complex, but those complications were tuned through time.
Next milestone was introduction of multi-CPU machines with separated memory for each physical unit.

Compared to functional testing, performance isn't evaluated as true/false result, but relative change to previous measurement.
Due to this complexity is hard to use common tools for inspecting performance.
The biggest regression in scheduler doesn't hide in uneffective code, but in wrong placement of processes to cores and it's queues.

The usual testing method is simulating load simmilar to real usage.
To achieve this there are many benchmarks targetting different types of load, usually many parallel process, sometimes communicating between each other.

Good performance tests usually create great amount of data, which is required to be processed, aggregaed and visualised.

\chapter{Linux process scheduling}
Scheduler is a part of an operating system which assigns the processor time to tasks.
Its main goal is to maximize effectivity of the CPU usage and fairness of the
CPU time assigned to each task.

There are two opposing targets for a scheduler: either maximizing throughput or
minimizing latency. Lower amount of context switches leaves more CPU time for
tasks, but raises the response time on system events.
While user's workstation aims for a low response time, computational servers
require high throughput. Scheduler can be usually tuned to fit the intended
purpose.

In this chapter we describe basic behavior of Linux process scheduler. Then we
compare uniform and non-uniform memory access on multiprocessor architectures
and how scheduler handles them.

\section{Completely Fair Scheduler}
Completely Fair Scheduler is the current Linux process scheduler, which was
merged into the version 2.6.23 of the Linux kernel in 2007. Its author is Ingo
Molnár, who is the author of the previous \emph{O(1) scheduler} as well.

It features queuing tasks in a red-black tree structure ordered by time spent
running on the CPU so far. Red-black tree is a binary search
tree with self-balancing mechanism based on marking nodes with either red or
black color.
When the scheduler needs to choose the next task to run, it
takes the leftmost node with the lowest execution time. 

The time complexity of the CFS scheduling is O(log N). Taking the leftmost node
with the next task can be done in a constant time, but queuing the task again
requires O(log N) operations to insert it back into the red-black tree. Even
with a higher scheduling complexity, the CFS scheduler has a better fairness and
responsiveness than the previous O(1) scheduler, which used a simple queue to
choose the next task.
% CFS can also handle priority better: it requires only one queue instead of two
% for each of 140 priorities in Linux for O(1)

On multi-core systems, the scheduler uses a separate queue for each core. In
order to effectively use the processing power, the scheduler must regularly
balance those queues by moving processes from the most busy cores to idle ones.

When moving processes between cores, scheduler takes in the account a topology
of the system. Loosing data from caches after a migration can have a bigger
impact on performance than leaving the process on the busy core.

% Linux kernel book
CFS solves this problem by using scheduling domains. Scheduling domain is a set
of CPUs that should be balanced between themselves. CPUs in a scheduling domain
are divided into groups. Scheduler checks load of the whole groups to decide if
there is a need to migrate processes between them.

There are multiple levels of scheduling domains with different parameters such as
how often is the load difference checked or how big the load difference between
groups must be to migrate tasks to balance the queues.
The lowest level is between hyper-threaded logical cores where are almost no
losses of cached data and rebalancing can be done very often.
A higher level is between physical processor cores where  cache losses can have
bigger impact on the decision.
Above cores can be processor sockets on machines with multiple physical
processors with different access speed to different memory sections.

Scheduling domains are regularly rebalanced by going up from bottom of the
scheduling domain hierarchy and by checking balance of the groups on each level.

% https://lwn.net/Articles/764482/
\begin{figure}
  \centering
  \includegraphics[width=4cm]{obrazky-figures/sched-domains}
  \caption{Hiearchy of scheduling domains}
  \label{fig:sched-dom}
\end{figure}

\section{Scheduling on SMP systems}
Symmetric multiprocessing (SMP) is an architecture of computers with multiple
physical processors that have a single shared memory, a shared access to all IO
devices, and that run on the same instance of operating system. This allows the
machine to offer more processing power with a little overhead caused by memory
sharing.

Each processor has still its own high speed cache, but due to memory sharing,
\emph{cache coherence} must be maintained -- the data shared between processors
in their caches must be uniform.

There are two ways of accessing the shared memory from multiple processors:
uniform (UMA) and non-uniform (NUMA) memory access.

\subsection{Uniform memory access}
In the UMA architecture, all processors share a single memory controller that
they use to access the shared memory. Therefore, each processor has the same
speed of memory access and the same latency. They share a common access route to
the memory, which brings more simplicity at the cost of a lower bandwidth and
speed.

In this architecture, it is easier for the scheduler to balance processes
between the physical processors. The time to access the shared memory is same on
all of the cores and therefore there is no need to move the memory associated
with a process to any other place for faster access.

\subsection{Non-uniform memory access}
The NUMA architecture tries to solve the problem with low bandwidth.
It arranges physical processors or cores into nodes, where each node has its
own separate memory and a bus for faster access. This significantly improves
overall memory throughput of the system when used correctly.

Nodes are also have to be connected to each other to access memory of other
nodes. That is achieved using either interconnecting buses or controllers. Each
manufacturer has its own technology implementing the interconnection: Intel uses
Ultra Path Interconnect which replaced its QuickPath Interconnect from older
machines, AMD uses Infinity Fabric supersetting the older HyperTransport.

On bigger machines with the large amount of processors, not every two processors
are necessarily connected. Instead, they may but access the data through a path
of connected ones. This can be seen on Figure \ref{fig:proliant} showing an 8
NUMA node machine with an advanced structure of interconnect buses and
controllers.

%Best Practices When Deploying Linux on DL980 4AA3-6556ENW.pdf
\begin{figure}
  \centering
  \includegraphics[width=14cm]{obrazky-figures/proliant}
  \caption{Architecture of NUMA communication on HP ProLiant DL980. Each
    processor connected with its own local memory makes up a NUMA node. Each
    pair of nodes have dedicated interconnect bus for faster data transfer
    between them. Communication with other nodes is realized through node
    controllers. The topology of the interconnect buses shows there will be 4
    different access speeds depending on the distance between nodes. The local
    access will is the fastest, then the access to the neighbor node, access
    through one controller and the slowest access through both controllers. The
    interconnect buses are doubled to avoid overloading one of them.}
  \label{fig:proliant}
\end{figure}

Consequence of interconnection between NUMA nodes is different latency between
nodes which must be taken into account when balancing tasks between nodes.
Difference in the access latency between pair of NUMA nodes for the machine from
Figure \ref{fig:proliant} can be seen in a part of output from a command
\verb|numactl --hardware|:

\begin{minipage}{\linewidth}
\begin{verbatim}

node distances:
node 0 1 2 3 4 5 6 7
0: 10 12 17 17 19 19 19 19
1: 12 10 17 17 19 19 19 19
2: 17 17 10 12 19 19 19 19
3: 17 17 12 10 19 19 19 19
4: 19 19 19 19 10 12 17 17
5: 19 19 19 19 12 10 17 17
6: 19 19 19 19 17 17 10 12
7: 19 19 19 19 17 17 12 10
\end{verbatim}
\end{minipage}\\

Balancing tasks between NUMA nodes is difficult for the scheduler since it needs
to take into account an expensive memory movement or access to different nodes.
With a wrong approach the performance of a NUMA system can drop even below the
performance of a similar UMA
system\footnote{http://highscalability.com/blog/2013/5/30/google-finds-numa-up-to-20-slower-for-gmail-and-websearch.html}.

Balancing processes between NUMA nodes is still in active development, which
brings many changes. These usually improve performance, however, there are cases
when a change can cause a performance regression. Therefore, it is essential to
carry out a thorough performance testing of the scheduling.

% TODO task can be pinned

% \begin{figure}
%   \centering
%   \includegraphics[width=15cm]{obrazky-figures/lstopo}
%   \caption{Topology of 4 NUMA node machine generated using lstopo utility}
% \end{figure}


\chapter{Performance measurement}
Performance testing is examination of the system behavior under workload and its
effectivity of resource usage. Many systems depend on the time they are able to
respond and this property affects the usability of th system.

Compared to functional testing, performance testing does not produce exact true
or false result. It produces set of numerical values which must be compared to
pressumed values or values from other version to make a conclusion.

Next step is inspection of behavior of the system to understand the measured
values and determine the causes of the difference from the expected values.

For evaluation of system performance are usually used benchmarks.
Benchmarks generate artificial load imitating the load in real environment.
While stressing the system they also measure the performance. The benchmarks
typically return a value representing the performance of the system. The value
is usually in the form of the time that the task needed to finish or amount of
the operations that the system could perform per a unit of time.

Effectivity of task scheduling and of their migration between processors affects
the amount of the tasks that the system can handle and the time that a task
spends before finishing.

Although the benchmarks generate values that are suitable for comparison, they
do not provide any more detailed information about how the system achieved the
measured performance and where could be some possible bottlenecks.

To get a better insight into the behavior of the system, there are many tools to
collect information about the system behavior. Useful informations about the
scheduler include assignment of tasks to the processor cores, the time that the
tasks spent out of the CPU in queues, load of each processor core, and the
location of memory of the processes on NUMA systems.
% TODO cite http://www.brendangregg.com/activebenchmarking.html

\section{Performance metrics}
run time

throughput

\section{Benchmarks}
In this chapter we will describe the benchmarks that are currently used to evaluate
performance of the latest kernel versions. The benchmarks are usually based on real
applications used both in scientific and in business environments.

The benchmarks run in many threads or processes and feature communication
between the INSTANCES. A bad distribution of processes and threads by the
scheduler causes their time of waiting in the queue to rise and the performance of the
system to go down. Moreover, on NUMA systems, the performance depends on placement of
data in the memory.

\subsection{NAS Parallel Benchmarks}
NAS Parallel Benchmarks is a set of benchmarks focused on performance of highly
parallel computations on supercomputers. In addition to floating point
computations, it targets communication and data movement among computation
nodes. The performed algorithms are based on large scale computational fluid
dynamics at the Numerical Aerodynamic Simulation (NAS) Program which is based at
NASA Ames Research Center.

Benchmarks are written in Fortran-90 or in C language, because these were the most
commonly used programming languages in scientific parallel computing community
at the time when the benchmarks were created. They can be compiled with different classes of
problem sizes to suit machines with different amount of memory and of computational
power.

The main output value of the benchmark is throughput measured in units called Mop/s
(millions of operations per second) representing the amount of floating-point
operations per unit of time.

The benchmark also offers a few parameters that can be passed to the benchmark
before the execution to tweak properties of its behavior. One of them is a number of
computation threads, which in a lower amount slowers the run time, but allows to
measure behavior of the system without full usage. Figure \ref{fig:nas} shows
an example of a throughput with different number of threads on a machine with 24 physical cores
and Hyper-threading.

A downside of this benchmark is it can only run with a fixed dataset, but not for a
fixed time period. This constraint makes the run time of the benchmark with less
threads longer.

\begin{figure}
  \centering
  \includegraphics[width=12cm]{obrazky-figures/nas}
  \caption{Example of Scalar Penta-diagonal solver results from NAS Parallel with
    different number of computational threads.}
  \label{fig:nas}
\end{figure}

\subsection{SPECjbb2005}
Java Business Benchmark behaves as a server-side Java application. It is
primarily focused on measuring performance of Java implementation, but it also
reflects performance of operating system and of the CPU itself.
It models a system of a wholesale company as a multitier application. The benchmark
itself creates the load, measures the throughput, and also generates a simple
report in HTML and raw text formats.

The main output value is \emph{throughput} measured in units called
\emph{SPECjbb2005 bops} \footnote{Business operations per second}. In case of
using more JVM\footnote{Java virtual machine} instances, there is a second unit
called SPECjbb2005 bops/JVM representing an average throughput of a single JVM
instance. Another collected metric is memory a consumption, which isn't that
useful in scheduler performance monitoring.

\subsection{SPECjvm2008}
SPECjvm2008 is benchmark primarily focused on performance of Java Virtual
Machine, but its results can reflect performance of the scheduler.

\subsection{LINPACK benchmark}
LINPACK Benchmark comes from LINPACK package, which was used to solve systems of linear equations in single or double precision.

\subsection{Stream benchmark}

\section{Performance analysis tools}

\subsection{time}
Time is a simple command for measuring the time that an application spent running. The
most common numbers it gives out are the total of real time that the application needed
to finish, the time that it spent in the user mode, and the time spent in the kernel mode.

Many benchmarks provide the execution time themselves which can make this
utility look useless. However the intersting information is the difference between
the userspace and the kernel time and the total execution time, which is time that
the application spent out of the CPU waiting in a queue.

It can be confused with bash builtin command \texttt{time}, which provides similar
information, but the real command can provide more verbose information with
a possibility of custom formating of output. It can be usually called from
\texttt{/usr/bin/time}.

\subsection{ps}
Ps is Linux command, which is used to display about active processes. Its name
stands for processes status. It can provide various information obtainable from
the virtual files in \texttt{/proc} directory. The most common information
include their PID, time spent on processor, state of the process, used memory,
associated terminal, the command that started the process and more.

Especially useful are optional columns \texttt{PSR} and \texttt{NUMA}. They
show the number of processor and the number of NUMA node where the process is
running. Continuous monitoring of those values can provide view on migration of
the process during its run time. 

Output of the command can be filtered in many ways. By default it shows only
processes from the current user and the current terminal, but it can list all
the processes on the system. The listing can be filtered using parameters by
most of the columns of information it provides. The listing can be limited for
instance by specific terminal, effective user, children of a specified process
or PID to single intended process.

\subsection{mpstat}
This Linux command provides continuous information about utilization of CPUs. It
can show utilization of processor cores, NUMA nodes or the whole system
dependent on passed argument. Some of the provided values are utilization in
user space, utilization in kernel space, percentage of waiting for IO
operations, percentage of handling interrupt requests and the idle percentage,
when system is idle and does not wait for any IO operation.

Mpstat can collect those data once when executed or at regular time intervals.
The regular collecting of utilization of the CPU cores or NUMA nodes through the
run time of benchmark. With little processing of the data is easy to watch the
equal or unequal distribution of load between the cores and NUMA nodes.

\subsection{turbostat}
This tool provides measuring hardware properties of the CPUs with x86
architecture. It reports for each core its usage, frequency, temperature,
percentage of time in idle states. For each socket it reports power consumption.

There are two ways to run turbostat. It can be supplied with a command to run
and it will return the average values from the run time of the command. Without
the command it will collect the statistics at regular time intervals.

Data from this tool can be used to analyze performance drop caused by CPU
itself. It can be due to frequency drop because of overheating or missing
workload. The power consumption data can be used to roughly compare the power
efficiency of both scheduler and physical CPUs, but provide only consumption of
the CPUs and their RAM and not the whole machine.

\subsection{perf}


\chapter{Storing the results}
Benchmarks usually generate long human-readable output in text or even HTML
format. This is useful when analyzing single report. In the output are details
of the test run itself, simple resource usage or success of result validation.
However, the amount of result starts to rise with repeated runs, different
amount of instances and new versions kernels.

For the comparison of performance results, it is usually enough the number
representing throughput or time of each benchmark run. Those numbers can be
preprocessed from the benchmark output files to a format more suitable for quick
accesing required data.

\section{XML files}
XML is markup language, that can store heterogeneous data in tree structure.
The tree structure can effectively represent the test scenario running each
benchmark operation with different parameters and multiple repetitions.

Another feature of XML format is human-readability offering quick insight to stored
data just with any text editor.
This comes with a disadvantage of redundant data in form of repeated names of
tags and attributes which often take more space than the data itself. Parsing of
the data also take considerable amount of CPU time prolonging the duration of
analysis.

Example of results stored in XML format:
\begin{verbatim}
<?xml version="1.0"?>
<BeakerRunResult>
 <TestResult>
  <NASResult benchmarkName="lu_C_x">
   <threads number="002">
    <result timeReal="293.764" timeOutOfCPU="10.789" mops="7016.79"/>
    <result timeReal="291.454" timeOutOfCPU="7.086" mops="7075.39"/>
    <result timeReal="290.403" timeOutOfCPU="7.844" mops="7096.21"/>
    <result timeReal="288.959" timeOutOfCPU="4.079" mops="7131.56"/>
    <result timeReal="289.642" timeOutOfCPU="4.644" mops="7118.24"/>
   </threads>
   <threads number="004">
    <result timeReal="153.149" timeOutOfCPU="12.688" mops="13462.15"/>
    <result timeReal="154.477" timeOutOfCPU="18.743" mops="13344.54"/>
...
\end{verbatim}
This example shows beginning of XML file with important values from one NAS
Parallel benchmark run scenario. The XML file starts with root element
\texttt{<BeakerRunResult>} and \texttt{<TestResult>} node which are wrapping
\texttt{<NASResult>} nodes representing results from each benchmark operation
from NAS Parallel benchmark suite. Each benchmark operation is run with
different amount of threads in few loops to lower the measurement inaccuracy.
Nodes of results with the same number of threads are wrapped in
\texttt{<threads>} node. All the values are stored as attributes of the
corresponding node.

\section{Database}


\chapter{Displaying the results}
\section{Heat maps}
Heat maps are three dimensional graphs which are using color as the third
dimension for values. This allows to plot two dependencies of the values
compared to line graphs, which must use multiple lines to plot the same data.
Heatmapy škálují pro velké data. 
Data jsou kvantizovány do bucketů.

In the Figure \ref{fig:heatmap} is heat map showing utilization of all CPU cores over
time under workload. The data were collected by mpstat utility and processed to
show sum of user and kernel space utilization of each core. Plotting those data
using line graph with a line for every core would be confusing even for this
relatively small amount of CPUs.

\begin{figure}
  \centering
  \includegraphics[width=14cm]{obrazky-figures/heatmap}
  \caption{Example of heat map showing CPU utilization over time. The machine
    with 24 logical CPUs is under workload from NAS Parallel benchmark running
    in 16 threads.}
  \label{fig:heatmap}
\end{figure}

Another use of heat map is shown in Figure \ref{fig:numa_heatmap}. It does not
show utilization of threads, but their location on which NUMA node they are.
This heat map shows the migration of threads between NUMA nodes and the expected
result is not the highest value, but minimum of color changes in each line. The
shown data comes from NAS Parallel benchmark, which was run with 16 and 24
threads in 5 loops. The heat map shows better scheduler behavior on the 16
threads run than on the 24 threads run.

\begin{figure}
  \centering
  \includegraphics[width=14cm]{obrazky-figures/numa_heatmap}
  \caption{Example of heat map showing thread migration between NUMA nodes. The
    expected result is not the highest value, but minimum of color changes in
    each line. The shown result comes from machine with 24 logical CPUs running
    NAS Parallel benchmark on 16 and 24 threads in 5 loops.}
  \label{fig:numa_heatmap}
\end{figure}

\section{Box plots}
Box plot is method for displaying statistical properties of data from multiple
measurements. It extends the simple visualization of discrete values by adding
to the median values also the minimum and maximum measured values and first and
third quartiles from the measurement. In special cases are also added marks for
99\textsuperscript{th} or similar percentile relevant for the given case.

All the displayed values shows the accuracy and reliability of measurement. This
insight helps to distinguish real performance regression from noise caused by
unpredictable behavior of the scheduler and measurement error.

In the Figure \ref{fig:boxplot} is an example of box plot showing throughput
measured by NAS benchmark with different number of threads.  

\begin{figure}
  \centering
  \includegraphics[width=12cm]{obrazky-figures/boxplot}
  \caption{Example of boxplot showing throughput measured by NAS benchmark from
    multiple measurements with different number of threads.}
  \label{fig:boxplot}
\end{figure}


\chapter{Timelines}
An common way to get performance report is to compare two results. Usually, they
are called baseline and target results. The comparison of two results allows to
write many details about the measurement and changes between versions. Those
details usually contain clues to the cause of possible performance change.

However, sometimes is not enough to compare just two versions and larger amount
of results over longer period of time can bring a new perspective. There is much
more visible difference between the deviation from measurement error and the
performance change. It is also easier to find the versions, where a
performance degradation occured and where was fixed.

With larger amount of data, there can also emerge performance drops, which
appeared continuously over longer period of time and couldn't be spotted,
because they were in tolerance due to deviation.

\section{Result storage}
Benchmark results are stored on filesystem in directories. Each result consists
of two XML files with information and preprocessed main data from the test run,
more files with larger complementary data from analysis tools and compressed
original outputs from benchmark. First XML file contains metadata from the test
run including time, machine hostname, kernel and OS version, benchmark name,
configuration of environment, which could affect the result and few other data.
The second XML file contains the important preprocessed data itself.

The program has to go throgh all these results to choose the right ones for the
following creation of graphs.

\section{Comparison rules}
For automatic report generation are essential rules, which will specify results,
that can be used and in which role. I chose \emph{regular expressions} to match
properties of results. Regular expressions offer broad possibilities to describe
shape of kernel version or just value of any evironment configuration. To filter
all builds of kernel 4.18 works simple pattern \texttt{kernel-4.18\textbackslash
..*}.

To store the rules are stored in XML file with same node naming as in XML file
with result properties. The first level of XML document contains three nodes
representing purpose of rules.
\begin{itemize}
  \item \textbf{Baseline rules} specify the first result in plotted set. Acts as
    main result others are compared to.
  \item \textbf{Target rules} define the results to be plotted.
  \item \textbf{Starting rules} are for case, when base result is not from
    target set of results and specifying first target result with regex would be
    hard.
\end{itemize}
Each of these nodes then contain in them nodes with the rules.


\chapter{Automatic evaluation}
With every expansion of regular kernel testing rises the amount of produced
results. With more machines with different configurations, benchmarks with
different focus or baselines from different supported versions, number of results
can rise with every new kernel version even by hundreds. This tends to automate
the repetitive classification of results between pass and fail to leave results
without significant change and focus on the ones with performance regressions.

\section{Marking of results}
To teach automatic classifier is essential large amount of data. To reduce
time spent on marking of the passed and failed results I created a form in
report to quicken this process.

On machine for storing results is running simple server application for
committing requests from the form. The server is written in Python built on top
of Flask framework. On request it finds given result and modifies its file with
marking for learning.

\begin{figure}
  \centering
  \includegraphics[width=12cm]{obrazky-figures/teaching_table}
  \caption{Form from HTML comparison report page to mark data for machine learning}
\end{figure}

\section{Human classification of results}
There are quite many results to check with every new tested kernel. The test
suite with the Linpack, Stream, NAS Parellel and SPEC benchmarks are run on more
than eight machines with different amount of NUMA nodes and processor models.
Man has to go through the reports and check the results of all the benchmarks
with different configurations.

When looking at results of benchmark operation, the most important values are
medians from runs with different amount of threads. Unfortunately, due to noise in measurement
and limited amount of repeated runs the medians can be significantly affected by
the noise. This complication brings the need of more detailed inspection of
results than just watching difference in medians of results.

Another useful data are minimum, maximum and quartile values from each
measurement. They reveal the stability of the benchmark and the measurement
noise. With those values it is much easier and more accurate telling the
measurement noise from performance regression. With similar minimum, maximum and
quartile values the performance can be the same even with difference in median values.

The threshold between noise and performance regression is considered as 5\%
difference between base and target measurement but varies by the stability of
each benchmark and complexity of the machine it was run on.

\section{Used technologies}
For classification we will use library scikit-learn for python3. Scikit-learn is
simple library for data analysis and machine learning with BSD open source
license. It is easy to use with fast learning curve and well documented. It is
good choice for small and medium sized projects that do not need massive
scalability. It provides various algorithms for classification, regression and
clustering built on NumPy and SciPy python libraries.

Unlike other machine learning libraries like PyTorch and TensorFlow the
scikit-learn library does not focus on deep neural networks for larger and
advanced problems. It provides more simple classifiers for easier problems
with smaller datasets where advanced methods would not have enough training
data.

\section{Reading the results}
Learning data come as an XML file containing preprocessed data from
base and target run of the benchmark which are marked as passed or failed.

Record of each benchmark operation is marked as pass or fail and contains
records of runs with different amount of threads or processes. Those records
contain median, minimum, maximum and quartiles form the repeated runs of the
configuration. The root element contains UUIDs of the comparison and its base
and target results for easier tracing in case of suspicious values.

Here is an example of data for the teaching of classifiers:
\begin{verbatim}
<comparison_results base_uuid="..." report_uuid="..." target_uuid="...">
  <comparison_result benchmark_name="NASParallel"
      operation_name="bt_C_x" result_status="pass">
    <Result median_diff="1" threads_no="4">
      <base_result first_q="7409.2" max="7507.0" mean="7451.7"
        median="7456.5" min="7406.4" stdev="39.2" third_q="7479.3" />
      <target_result first_q="7503.0" max="7597.5" mean="7513.1"
        median="7522.2" min="7404.8" stdev="62.7" third_q="7537.8" />
    </Result>
    <Result median_diff="0" threads_no="8">
      <base_result first_q="14330.8" max="14740.1" mean="14476.6"
        median="14469.1" min="14325.9" stdev="151.7" third_q="14517.2" />
      <target_result first_q="14494.5" max="14619.3" mean="14481.1"
        median="14521.6" min="14233.7" stdev="130.5" third_q="14536.5" />
    </Result>
...
\end{verbatim}

\section{Preprocessing for learning}
\section{Comparison of classifiers}
Classification is procedure of assigning category to new observation based on
training set of observations with specified category. Because of the
availability of already classified data it belongs to \emph{supervised learning}
part of machine learning.

Data for classification are represented by set of vectors with fixed element
count. Vector is an ordered set of numbers where each one represents a value in
separate dimension of the source data. For supervised learning have vectors from
training set assigned categories (also called classes). In our case we will have
2 classes: \emph{pass} and \emph{fail}.

For the following training and evaluation of the models we have roughly 250
marked vectors. This should be enough to train different simple classifiers with
sufficient accuracy on other unmarked vectors.


%\chapter{Future work}


\chapter{Notes}
\begin{verbatim}
Linux scheduler
    CFS
    numa planning
        not easy for scheduler
        manual pinning option
            numactl
        group imbalance bug
    tune profiles
        focus on throughput or latency
Performance
    performance isn't simple pass/fail
    comparison of baseline and target kernel
    running just benchmark is waste of testing potential, it's good to collect more data
        http://www.brendangregg.com/activebenchmarking.html
    provisioning of machines with Beaker
    collecting system load data
        mpstat
            usage of every cpu
        numastat
        numatop
            usage of cpu and memory on each node
        ps
            psr and numa column
            cpu time
        time
            total, user and system time spent
        turbostat
        perf stat
        lstopo for hardware schema image
        free
    benchmarks
        specjbb2005
        specjvm2008
        nas parallel
        linpack benchmark
        stream (memory throughput)
        specjbb2015 - not stable
        hackbench - not tried yet
    scenarios
        variable number of benchmark instances
        with and without pinning processes to specific numa nodes
    result storing
        preprocessing to xml
        storing to database
        aggregating results
        computing statistical data
    plotting
        lstopo for plotting hardware topology
        bargraphs to show inaccuaracy
            operations per second (runtime of benchmark)
            time out of cpu
        detailed comparsion of two results
        timeline with many results over longer period of time
        heatmaps for core usage over time
            http://www.brendangregg.com/HeatMaps/utilization.html
        graph of process migration between numa nodes
Timelines
    analysis
    used technologies
    desing
    implementation
    output
Automatic regression detection
    motivation
    marking data
    preprocessing for classificator
    methods
        linear logistic regression
        decision trees
    evaluation
\end{verbatim}

\chapter{Specification}
\begin{enumerate}
\item Get acquainted with the existing methods for measuring performance of the Linux kernel scheduler and with means of storing of benchmarks results for further processing.
\item Study possible ways of processing these results with a focus on graphic interpretation and on methods for detection of performance degradation.
\item Design and implement a method for efficient graphic interpretation of long-term measurements.
\item Design and implement a method for automatic detection of performance regression.
\item Demonstrate the functionality of your implementation on at least two versions of the Linux kernel.
\item Evaluate the obtained results and discuss possibilities of further development of the project, especially of the automatic detection of performance regression.
\end{enumerate}

Literature:
\begin{itemize}
\item Lozi, Jean-Pierre, et al. "The Linux scheduler: a decade of wasted cores." Proceedings of the Eleventh European Conference on Computer Systems. ACM, 2016.
\item Daniel, P., and Cesati Marco. "Understanding the Linux kernel." (2007).
\item Bailey, David H., et al. "The NAS parallel benchmarks." The International Journal of Supercomputing Applications 5.3 (1991): 63-73.
\end{itemize}
