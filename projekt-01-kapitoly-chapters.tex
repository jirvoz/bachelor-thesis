\chapter{Introduction}
Performance of operating system is crucial as any degradation can significantly
affect the performance of all applications running above it. Moreover, when a
new version is released and it contains performance regression, it can even
break the stability of business applications leading to great financial losses.
Important part of systems is the implementation and strategy of process
scheduler, which manages processes and their processor time.

Scheduler used to be simple, but with introduction of multi-core CPUs came
requirement of multiple runqueues for each core and their balancing. This brought
some complexity to the code and took some time to eliminate bugs and tune the
behavior. Another change came with symmetric multiprocessing technology bringing
to market machines with multiple CPU sockets and non-uniform memory organization
and access. Scheduling on those systems is still under active development and
prone to performance degradation bugs.

Compared to functional testing, performance cannot be evaluated as just true and
false results. It is more challenging as we have to compare it relatively with
previous versions or measurements, and choose suitable threshold, when reporting
the performance regression. Due to complexity of the scheduler the common tools
for inspecting performance may not frequently yield satisfying results.
Moreover, the biggest performance regressions of the scheduler does not dwell in
inefficient code, but instead in inefficient placement of processes to the cores
and their queues.

The usual performance testing method of the scheduler is to simulate load
similar to real usage. Currently there are many benchmarks targeting different
types of load, usually spawning many parallel process, sometimes even
communicating between each other. The results of measurements of the performance
must be also stored systematically and effectively for later use in generation
of comparisons. For the right and effective interpretation of the collected
results and their comparisons is essential right type of their visualization.

In this thesis, we propose an utilization of machine learning methods that will
automatically check for possible degradations in Linux kernel scheduler. We will
describe creation of dataset for the classification, compare different
classifiers, and evaluate their accuracy on the dataset.
Moreover, we will interpret long-term results of the benchmarks using box plots
in order to enhance the process of data analysis. This visualization should help
with inspecting the measurement stability of benchmarks, finding versions where
performance degradations appeared or were fixed.

This thesis is structured as follows. In Chapter \ref{ch:scheduler} we describe
Completely Fair Scheduler -- the current process scheduler of Linux kernel,
architecture or symmetric multiprocessing systems, and how the scheduler handles
them. In Chapter \ref{ch:measurement} we describe the performance measurement of
the Linux process scheduler. First we describe benchmarks used in Red Hat for
load testing of scheduler performance, then complementary tools for analysis of
behavior of the scheduler and the system. Ways of storage of the collected data
we describe in Chapter \ref{ch:storage} and visualization of the results and
their comparisons in Chapter \ref{ch:visualization}. A new way of visualization
of long-term results comparison called \emph{Timelines} we propose in Chapter
\ref{ch:timelines}. In Chapter \ref{ch:ai} we propose utilization of machine
learning methods for automatic classification of comparison of two results to
recognize a performance regression.



\chapter{Linux process scheduling} \label{ch:scheduler}
Scheduler is a part of an operating system which assigns the processor time to tasks.
Its main goal is to maximize effectivity of the CPU usage and to ensure fairness of the
CPU time assigned to each task.

There are two opposing targets for a scheduler: either maximizing the throughput or
minimizing the latency. Lower amount of context switches leaves more CPU time for
tasks, but raises the response time on system events.
While users' workstations aims for a low response time, computational servers
require high throughput. Scheduler can then be usually tuned to fit the intended
purpose.

In this chapter we describe basic behavior of Linux process scheduler. Then we
compare uniform and non-uniform memory access on multiprocessor architectures
and how scheduler handles them.

\section{Completely Fair Scheduler}
Completely Fair Scheduler (CFS) is the current Linux process scheduler, which was
merged into the version 2.6.23 of the Linux kernel in 2007. Its author is Ingo
Molnár, who is the author of the previous \emph{O(1) scheduler} as well.

CFS features queuing tasks in a red-black tree structure ordered by time spent
running on the CPU so far. Red-black tree is a binary search
tree with self-balancing mechanism based on marking nodes with either red or
black color. When the scheduler needs to choose the next task to run, it takes
the leftmost node with the lowest execution time.

The time complexity of the CFS scheduling is O(log N), where N is the number
of running tasks. Taking the leftmost node
with the next scheduled task can be done in a constant time, but queuing the task
requires O(log N) operations to insert it back into the red-black tree. Even
with a higher scheduling complexity, the CFS scheduler has a better fairness and
responsiveness than the previous O(1) scheduler, which used a simple queue to
choose the next task.
% CFS can also handle priority better: it requires only one queue instead of two
% for each of 140 priorities in Linux for O(1)

On multi-core systems, the scheduler uses a separate queue for each core. In
order to effectively use the processing power, the scheduler must regularly
balance those queues by moving processes from the most busy cores to idle ones.

When moving processes between cores, scheduler takes in the account a topology
of the system. Losing data from caches after the migration can have a bigger
impact on performance than leaving the process on the busy core.

% Linux kernel book
CFS solves this problem by using scheduling domains. Scheduling domain is a set
of CPUs that should be balanced between themselves. CPUs in a scheduling domain
are divided into groups. Scheduler then checks the load of the whole groups to
decide if there is a need to migrate processes between them.

There are multiple levels of scheduling domains with different parameters such as
how often is the load difference checked or how big the load difference between
groups must be to migrate tasks to balance the queues.
The lowest level is between hyper-threaded logical cores where there are almost
no losses of cached data and rebalancing can be done quite often.
A higher level is between physical processor cores where cache losses can have
bigger impact on the decision to migrate the task.
Above those cores can be processor sockets on machines with multiple physical
processors with different access speed to different memory sections.

Scheduling domains are regularly rebalanced by going up from bottom of the
scheduling domain hierarchy and by checking balance of the groups on each level.

% https://lwn.net/Articles/764482/
\begin{figure}
  \centering
  \includegraphics[width=4cm]{sched-domains}
  \caption{Hiearchy of scheduling domains}
  \label{fig:sched-dom}
\end{figure}

\section{Scheduling on SMP systems}
Symmetric multiprocessing (SMP) is an architecture of computers with multiple
physical processors that have a single shared memory, a shared access to all IO
devices, and that run on the same instance of operating system. This allows the
machine to offer more processing power with a little overhead caused by memory
sharing.

Each processor has still its own high speed cache, but due to memory sharing,
\emph{cache coherence} must be maintained -- the data shared between processors
in their caches must be uniform.

There are two ways of accessing the shared memory from multiple processors:
uniform (UMA) and non-uniform (NUMA) memory access. The NUMA technology is more
widespread thanks to higher performance capability with similar configuration.
% TODO motivate the NUMA more - usage in RH

\subsection{Uniform memory access}
In the UMA architecture, all processors share a single memory controller that
they use to access the shared memory. Therefore, each processor has the same
speed of memory access and the same latency. They share a common access route to
the memory, which brings more simplicity at the cost of a lower bandwidth and
speed.

In this architecture it is easier for the scheduler to balance processes
between the physical processors. The time to access the shared memory is the
same on all of the cores and therefore there is no need to move the memory
associated with a process to any other place for faster access.

\subsection{Non-uniform memory access}
The NUMA architecture tries to solve the problem with low bandwidth.
It arranges physical processors or cores into nodes, where each node has its
own separate memory and a bus for faster access. This significantly improves
overall memory throughput of the system when used correctly.

Nodes also have to be connected to each other to access memory of other
nodes. That is achieved using either interconnecting buses or controllers. Each
manufacturer has its own technology implementing the interconnection: Intel uses
Ultra Path Interconnect which replaced its QuickPath Interconnect from older
machines. On the contrary AMD uses Infinity Fabric supersetting the older
HyperTransport.

On bigger machines with a large amount of processors, not every two processors
are necessarily connected. Instead, they may access the data through a path
of connected nodes. This can be seen in Figure \ref{fig:proliant} showing an 8
NUMA node machine with an advanced structure of interconnect buses and
controllers.

%Best Practices When Deploying Linux on DL980 4AA3-6556ENW.pdf
\begin{figure}
  \centering
  \includegraphics[width=14cm]{proliant}
  \caption{Architecture of NUMA communication on HP ProLiant DL980. Each
    processor connected with its own local memory makes up a NUMA node. Each
    pair of nodes have dedicated interconnect bus for faster data transfer
    between them. Communication with other nodes is realized through node
    controllers. The topology of the interconnect buses shows there will be 4
    different access speeds depending on the distance between nodes. The local
    access is the fastest, then follows the access to the neighbor node, the
    access through one controller and the slowest is the access through both
    controllers. The interconnect buses are doubled to avoid overloading one of
    them.}
  \label{fig:proliant}
\end{figure}

Consequence of interconnection between NUMA nodes is the different latency
between nodes which must be taken into account when balancing tasks between
nodes. Difference in the access latency between a pair of NUMA nodes for the
machine from Figure \ref{fig:proliant} can be seen in the following part of
output from a command \verb|numactl --hardware|:

\begin{minipage}{\linewidth}
\begin{verbatim}

node distances:
node 0 1 2 3 4 5 6 7
0: 10 12 17 17 19 19 19 19
1: 12 10 17 17 19 19 19 19
2: 17 17 10 12 19 19 19 19
3: 17 17 12 10 19 19 19 19
4: 19 19 19 19 10 12 17 17
5: 19 19 19 19 12 10 17 17
6: 19 19 19 19 17 17 10 12
7: 19 19 19 19 17 17 12 10
\end{verbatim}
\end{minipage}\\

Balancing tasks between NUMA nodes is difficult for the scheduler since it needs
to take into account an expensive memory movement or access to different nodes.
With a wrong approach the performance of a NUMA system can drop even below the
performance of a similar UMA
system\footnote{http://highscalability.com/blog/2013/5/30/google-finds-numa-up-to-20-slower-for-gmail-and-websearch.html}.

Balancing processes between NUMA nodes is still in active development, which
brings many changes. These usually improve performance, however, there are cases
when a change can cause a performance regression. Therefore, it is essential to
carry out a thorough performance testing of the scheduling.

% TODO task can be pinned

% \begin{figure}
%   \centering
%   \includegraphics[width=15cm]{lstopo}
%   \caption{Topology of 4 NUMA node machine generated using lstopo utility}
% \end{figure}



\chapter{Performance measurement} \label{ch:measurement}
Performance testing is examination of the system behavior under a workload and of its
effectivity of resource usage. For many systems, the time they are able to
respond in is crucial and this property affects the usability of the system.

Compared to functional testing, performance testing does not produce an exact true
or false result. It produces a set of numerical values which must be compared to
pressumed values or values from an other version to make a conclusion.

After the performance measurement, the next step is inspection of behavior of
the system to understand the measured values and to determine the causes of the
difference from the expected values.

In order to measure the performance of scheduler, we use benchmarks.
Benchmarks generate artificial load imitating the load in real environment.
While stress testing the system they also measure its performance. The benchmarks
typically return a value representing the performance of the system. The value
is usually in the form of the time that the task needed to finish or of the amount of
the operations that the system could perform per a unit of time.

In previous chapter, we have introduced the current state of task scheduling in
Linux kernel.
Effectivity of the scheduling and of their migration between processors affects
the amount of the tasks that the system can handle and the time that a task
spends before finishing.

Although the available benchmarks generate values that are suitable for comparison, they
do not provide any more detailed information about how the system achieved the
measured performance or where the possible bottlenecks could be.

To get a better insight into the behavior of the system, there are many tools to
collect performance records about the system behavior. Useful information about the
scheduler include assignment of tasks to the processor cores, the time that the
tasks spent out of the CPU in queues, load of each processor core or the
location of memory of the processes on NUMA systems.
% TODO cite http://www.brendangregg.com/activebenchmarking.html

\section{Performance metrics}
run time

throughput

\section{Benchmarks}
In the following section we will describe the benchmarks that are currently used
to evaluate performance of the latest kernel versions. The benchmarks are
usually based on real applications used both in scientific and in business
environments and are meant to stress test the system.

The benchmarks run in many threads or processes and feature communication
between them to utilize communication between processors. A bad distribution of
processes and threads by the scheduler increases their time of waiting in the
queue to rise and the performance of the system naturally goes down. Moreover,
on NUMA systems, the performance depends also on placement of data in the
memory.

\subsection{NAS Parallel Benchmarks}
NAS Parallel Benchmarks is a set of programs focused on performance of highly
parallel computations on supercomputers. In addition to floating point
computations, it targets communication and data movement among computation
nodes. The performed algorithms are based on large scale computational fluid
dynamics at the Numerical Aerodynamic Simulation (NAS) Program which is based at
NASA Ames Research Center.

Benchmarks are written in Fortran-90 or in C language, as these were the most
commonly used programming languages in scientific parallel computing community
at the time when the benchmarks were created. They can be compiled with different classes of
problem sizes to suit machines with different amount of memory and of computational
power.

The main output value of the benchmark is the throughput measured in units called Mop/s
(millions of operations per second) representing the amount of floating-point
operations per unit of time.

The benchmark also offers a few parameters that can be passed to the benchmark
before the execution. One of them is the number of computation threads, which in
a lower amount slows down the run time, but allows one to measure behavior of
the system without full usage. Figure \ref{fig:nas} shows
an example of a throughput with different number of threads on a machine with 24 physical cores
and hyper-threading.

The downside of this benchmark is it can only run with a fixed dataset, but not for a
fixed time period. This constraint makes the run time of the benchmark with less
threads longer.

\begin{figure}
  \centering
  \includegraphics[width=12cm]{nas}
  \caption{Example of Scalar Penta-diagonal solver results from NAS Parallel
    benchmark with different number of computational threads. The size of the
    boxes represent the inaccuracy of measurement caused by noise and
    non-deterministic behavior of scheduler.}
  \label{fig:nas}
\end{figure}

\subsection{SPECjbb2005}
Java Business Benchmark behaves as a server-side Java application. It is
primarily focused on measuring performance of Java core implementation, but it
also reflects a performance of operating system and of the CPU itself. It models
a system of a wholesale company as a multitier application. The benchmark itself
creates the load, measures the throughput, and also generates a simple report in
HTML and raw text formats.

The main output value is \emph{throughput} measured in units called
\emph{SPECjbb2005 bops} \footnote{Business operations per second}. In case we
use more JVM\footnote{Java virtual machine} instances, there is a second unit
called SPECjbb2005 bops/JVM representing an average throughput of a single JVM
instance. The collected metric is memory consumption, which is not that
important for scheduler performance monitoring.

\subsection{SPECjvm2008}
SPECjvm2008 is benchmark primarily focused on performance of Java Virtual
Machine, but its results can reflect performance of the scheduler.

\subsection{LINPACK benchmark}
LINPACK Benchmark comes from LINPACK package, which was used to solve systems of linear equations in single or double precision.

\subsection{STREAM benchmark}
STREAM is the de facto industry standard benchmark
for measuring sustained memory bandwidth.

\section{Performance analysis tools}

\subsection{time}
Time is a simple command for measuring the time that an application spends
running. The most common numbers it reports are the total real time that the
application needed to finish, the time that it spent in the user mode, and the
time spent in the kernel mode.

Many benchmarks provide the execution time themselves which can make this
utility unnecessary. However, the interesting metric is the difference between
the userspace and the kernel time and the total execution time, which is the time that
the application spent out of the CPU waiting in a queue.
% TODO clarify this calculation

It can be confused with the Bash builtin command \texttt{time}, which provides similar
output, but the real binary can provide more verbose information with
the possibility of custom formatting of output. It can be usually called from
\texttt{/usr/bin/time}.

\subsection{ps}
Ps is a Linux command, which is used to display information about active processes. Its name
stands for ``processes status''. It can provide various information obtainable from
the virtual files in \texttt{/proc} directory. The most common information
include the process PID, time spent on processor, state of the process, used memory,
associated terminal, the command that started the process and more.

Especially useful are the optional columns \texttt{PSR} and \texttt{NUMA}. They
show the number of the processor and the number of the NUMA node where the process is
running. Continuous monitoring of those values can provide view on migration of
the process during its run time. 

Output of the command can be filtered in many ways. By default, it shows only
processes for the current user and for the current terminal, but it can list all
the processes on the system. The listing can be filtered using parameters by
most of the columns of information it provides. The listing can be limited for
instance by a specific terminal, by an effective user, by children of a specified process,
or by a PID a to single intended process.

\subsection{mpstat}
This Linux command provides continuous information about utilization of CPUs. It
can show utilization of processor cores, of NUMA nodes, or of the whole system
dependent on passed argument. Some of the provided values are utilization in
user space, utilization in kernel space, percentage of waiting for IO
operations, percentage of handling interrupt requests and the idle percentage,
when system is idle and does not wait for any IO operation.

Mpstat can collect those data once when executed or at regular time intervals.
The regular collecting of utilization of the CPU cores or of the NUMA nodes is
done through the run time of benchmark. With little processing of the data, it
is easy to watch the distribution of load between the cores and the NUMA nodes
is equal or not.

\subsection{turbostat}
This tool provides measuring of hardware properties of the CPUs with x86
architecture. It reports for each core its usage, frequency, temperature and
percentage of time in idle states. For each socket it reports its power consumption.

There are two ways to run turbostat. It can be supplied with a command to run
and it will return the average values from the run time of the command. Without
the command it will collect the statistics at regular time intervals.

Data from this tool can be used to analyze performance drop caused by the CPU
itself. This can happen due to frequency drop because of overheating or of
missing workload. The power consumption data can be used to roughly compare the
power efficiency of both the scheduler and the physical CPUs, but the command
only provides consumption of the CPUs and their RAM and not of the whole
machine.

\subsection{perf}
Linux tool for monitoring hardware and software events, profiling and tracing.

\subsection{numa* tools}
numactl
numatop
numastat



\chapter{Storing the results} \label{ch:storage}
Benchmarks sometimes generate long human-readable output in text or even HTML
format. This is useful when analyzing single report. In the output there are details
of the test run itself, simple resource usage or success of result validation.
However, the amount of result starts to rise with repeated runs, different
amount of instances and new versions kernels.

For the comparison of performance results, the number representing throughput or
time of each benchmark run is usually enough. Those numbers can be preprocessed
from the benchmark output files to a format more suitable for quick accessing
required data.

\section{XML files}
XML is a markup language, that can store heterogeneous data in tree structure.
The tree structure can effectively represent the test scenario running each
benchmark operation with different parameters and multiple repetitions.

Another feature of XML format is human-readability offering quick insight to stored
data just with any text editor.
This comes with a disadvantage of redundant data in form of repeated names of
tags and attributes which often take more space than the data itself. Parsing of
the data also take considerable amount of CPU time prolonging the duration of
analysis.

% TODO in our team we use this method for cases described in following figures

Example of results from one run of the NAS Parallel benchmark scenario stored in
XML format is in Figure \ref{fig:xml_result}. Another example with aggregated
data is in Figure \ref{fig:xml_sums}.

\begin{figure}
  \small
  \begin{verbatim}
<?xml version="1.0"?>
<beaker_run_result>
  <test_result>
    <nas_result benchmark_name="mg_C_x">
      <threads number="2">
        <result mops="5560.4" real_time="31.0" out_of_cpu_time="0.9"/>
        <result mops="5411.4" real_time="32.2" out_of_cpu_time="1.6"/>
        <result mops="5499.3" real_time="31.4" out_of_cpu_time="0.6"/>
        <result mops="5407.4" real_time="31.9" out_of_cpu_time="1.3"/>
        <result mops="5376.1" real_time="32.0" out_of_cpu_time="0.7"/>
      </threads>
      <threads number="4">
        <result mops="10254.9" real_time="16.8" out_of_cpu_time="1.2"/>
        <result mops="10075.4" real_time="17.1" out_of_cpu_time="1.7"/>
        <result mops="10226.6" real_time="16.8" out_of_cpu_time="1.4"/>
        <result mops="10250.2" real_time="16.8" out_of_cpu_time="1.3"/>
        <result mops="10227.7" real_time="17.0" out_of_cpu_time="2.5"/>
      </threads>
...
\end{verbatim}
  \normalsize
  \caption{This example shows beginning of XML file with important values from one NAS
    Parallel benchmark run scenario. The XML file starts with root element
    \texttt{<beaker\_run\_result>} and \texttt{<test\_run>} node which are wrapping
    \texttt{<nas\_result>} nodes representing results from each benchmark operation
    from NAS Parallel benchmark suite. Each benchmark operation is run with
    different amount of threads in few loops to lower the measurement inaccuracy.
    Nodes of results with the same number of threads are wrapped in
    \texttt{<threads>} node. All the values are stored as attributes of the
    corresponding node.}
  \label{fig:xml_result}
\end{figure}

\begin{figure}
  \small
  \begin{verbatim}
<?xml version="1.0"?>
<beaker_run_result>
  <test_result>
    <nas_result benchmark_name="mg_C_x">
      <threads number="2">
        <mops mean="5450.9" stdev="68.4" first_q="5407.4" median="5411.4"
          third_q="5499.3" max="5560.4" min="5376.1"/>
        <total_time mean="31.7" stdev="0.4" first_q="31.4" median="31.9"
          third_q="32.0" max="32.2" min="31.0"/>
        <out_of_cpu_time mean="1.0" stdev="0.4" first_q="0.7" median="0.9"
          third_q="1.3" max="1.6" min="0.6"/>
      </threads>
      <threads number="4">
        <mops mean="10207.0" stdev="66.8" first_q="10226.6" median="10227.7"
          third_q="10250.2" max="10254.9" min="10075.4"/>
        <total_time mean="16.9" stdev="0.1" first_q="16.8" median="16.8"
          third_q="17.0" max="17.1" min="16.8"/>
        <out_of_cpu_time mean="1.6" stdev="0.5" first_q="1.3" median="1.4"
          third_q="1.7" max="2.5" min="1.2"/>
      </threads>
...
\end{verbatim}
  \normalsize
  \caption{Another form of stored data shows this beginning of XML file. Instead
    of all values obtained from the benchmark run scenario, here are only
    aggregated statistical values form the sets of collected values from each
    configuration. The aggregated values include minimum, maximum, mean, median,
    quartiles and standard deviation of metrics like throughput, run time, or
    time in queue for CPU. Those values are directly usable for plotting of
    comparison graph without any manipulation with them lowering the time for
    generation of reports.}
  \label{fig:xml_sums}
\end{figure}

\section{Relational database}
Relational database is a type of database using relational model. The relational
model stores data in tables using rows for records and columns for their
attributes. Each row represents an unique record with its attributes in columns.
Columns store values of attributes with same data type. Record in different
tables can be connected in relationships.

There are many database management systems implementing the relational database
model available under various licenses. From the open-source we can name
PostgreSQL, SQLite, MySQL, or its fork MariaDB. To the category with proprietary
code belong implementations from companies including Oracle, Microsoft, or IBM.

Data in database are managed using SQL (Structured Query Language). It provides
commands for storing, manipulating and retrieving data. With advanced joining of
tables and filtering it provides wide possibilities of data processing just at
the point of reading of the stored data.

Database offers much faster access to data without complicated parsing of text
files. Searching through the data can be much faster with indexing of the
records by selected columns. Moreover, databases store data more
space-efficiently directly in binary format to avoid unnecessary conversions.

The efficiency of database comes with its disadvantages. 
Storing the data in binary form eliminates the possibility of quick insight to
data like with XML files. To access any data is required to write SQL query to
request specific from the storage.
More complications come with design of the database tables. Different benchmarks
produce different type of result and the requirements for stored data can change
over time. This requires building universal complex structures or occasional
changes in the database model.

In our team we consider the option of storing results of the benchmark in
database, considering the work needed for migration from storing in XML files.



\chapter{Displaying the results} \label{ch:visualization}
Effective analysis of results from performance measurement requires delivering
the comparison in a form in which can human quickly see the differences of
measured values and their severity. Visualization ...

It must deliver easily understandable data even to people that do not work with
performance analysis on their daily basis.

\section{Heat maps}
Heat maps are three dimensional graphs which are using color as the third
dimension for values. This allows to plot two dependencies of the values
compared to line graphs, which must use multiple lines to plot the same data.

Heatmapy škálují pro velké data. 
Data jsou kvantizovány do bucketů.

In the Figure \ref{fig:heatmap} is heat map showing utilization of all CPU cores over
time under workload. The data were collected by mpstat utility and processed to
show sum of user and kernel space utilization of each core. Plotting those data
using line graph with a line for every core would be confusing even for this
relatively small amount of CPUs.

\begin{figure}
  \centering
  \includegraphics[width=14cm]{heatmap}
  \caption{Example of heat map showing CPU utilization over time. The machine
    with 24 logical CPUs is under workload from NAS Parallel benchmark running
    in 16 threads.}
  \label{fig:heatmap}
\end{figure}

Another use of heat map is shown in Figure \ref{fig:numa_heatmap}. It does not
show utilization of threads, but their location on which NUMA node collected by
ps utility. This heat map shows the migration of threads between NUMA nodes and
the expected result is not the highest value, but minimum of color changes in
each line. The shown data comes from NAS Parallel benchmark, which was run with
16 and 24 threads in 5 loops. The heat map shows better scheduler behavior on
the 16 threads run than on the 24 threads run.

\begin{figure}
  \centering
  \includegraphics[width=14cm]{numa_heatmap}
  \caption{Example of heat map showing thread migration between NUMA nodes. The
    expected result is not the highest value, but minimum of color changes in
    each line. The shown result comes from machine with 24 logical CPUs running
    NAS Parallel benchmark on 16 and 24 threads in 5 loops.}
  \label{fig:numa_heatmap}
\end{figure}

\section{Box plots}
Box plot is a method for displaying statistical properties of data from multiple
measurements. It extends the simple visualization of discrete values by adding
to the median values also the minimum and maximum measured values and first and
third quartiles from the measurement. In special cases are also added marks for
99\textsuperscript{th} or similar percentile relevant for the given case.
% TODO why it displays those values

All the displayed values shows the accuracy and reliability of measurement. This
insight helps to distinguish real performance regression from noise caused by
unpredictable behavior of the scheduler and measurement error.

In the Figure \ref{fig:boxplot} is an example of box plot showing throughput
measured by NAS benchmark with different number of threads.  

\begin{figure}
  \centering
  \includegraphics[width=12cm]{boxplot}
  \caption{Example of boxplot showing throughput measured by NAS benchmark from
    multiple measurements with different number of threads.}
  \label{fig:boxplot}
\end{figure}



\chapter{Timelines} \label{ch:timelines}
Začlenit do předchozí kapitoly: první 2-3 způsoby popisují dosavadní metody
zobrazení a poslední je produktem této práce. Ukládání výsledků včlenit ještě
před zobrazování.

A common way to analyze performance reports is to compare two results measured
for different versions or settings. Usually, these
are called baseline and target results or profiles. The comparison of two
results allows one to
interpret many details about the measurement and changes between versions. Those
details usually contain clues to the cause of possible performance changes.

However, sometimes it is not enough to compare just two strictly following
versions. If instead we analyze larger amount
of results over longer period of time we can see a new perspective. In that case
there is much
more visible difference between the deviation from measurement error and the
performance change. It is also easier to find the versions, where exactly a
performance degradation occurred and where was fixed.

With larger amount of data, we can also detect creeping performance drops, which
appeared continuously over longer period of time and could not be detected,
because they were within the tolerance due to the measurement deviation.

\section{Design}
Main purpose is to watch performance through kernel versions on specific machine
with specific configuration like amount of memory or enablement of
hyper-threading. Each report will be generated for one testing machine
containing graphs of results from available benchmarks.

Best type of graph for results with repeated runs are boxplots. Boxplots show
important statistical values from the runs: median, minimum, maximum and first
and third quartiles. 

\section{Implementation}
highcharts for graphs

\section{Result storage}
Data jsou k dispozici v tomoto formátu.

Benchmark results are in our case stored on filesystem in directories. Each result consists
of two XML files with information and preprocessed main data from the test run,
more files with larger complementary data from analysis tools and compressed
original outputs from benchmark. First XML file contains metadata from the test
run including the time, machine hostname, kernel and OS version, benchmark name,
configuration of environment, which could affect the result.
The second XML file contains the important preprocessed data itself.

The analyzer has to go through all these results to choose the right ones for the
following creation of graphs.

\subsection{Comparison rules}
For automatic report generation are essential rules, which will specify results,
that can be used and in which role. We propose to use \emph{regular expressions} to match
properties of results. Regular expressions offer broad possibilities to describe
shape of kernel version or just value of any environment configuration. E.g. to filter
all builds of kernel 4.18 we can use simple pattern \texttt{kernel-4.18\textbackslash
..*}.

We store the rules in an XML file with the same node naming as in the XML file
with the result properties. The first level of XML document contains three nodes
representing the purpose of the rules.
\begin{itemize}
  \item \textbf{Baseline rules} specify the first result in the plotted set.
    Acts as the main result that the others are compared to.
  \item \textbf{Target rules} define the results to be plotted.
  \item \textbf{Starting rules} are for the case, when base result is not from
    target set of results and specifying first target result with regex would be
    hard.
\end{itemize}

\subsection{Highcharts}

\subsection{Parallelization}

\section{Output}
asdf



\chapter{Automatic evaluation} \label{ch:ai}
With every new release of regular kernel and its testing the amount of produced
results rises. With more machines with different configurations, benchmarks with
different focus or baselines from different supported versions, number of results
can rise with every new kernel version even by hundreds.

In this thesis we attempt to automate the repetitive classification and
labeling of results as passed (without any performance regression) or failed
(containing performance regression). This allows skipping the results without
any significant change. Instead, more time is left for focusing on the results
with performance regressions.

In this chapter we will describe the way of obtaining data from results and
their processing for the evaluation. We will describe different classification
models suitable for given dataset and compare their success rate.

\section{Human classification of results}
There are quite many results to check with every new tested kernel. The test
suite with the Linpack, Stream, NAS Parellel and SPEC benchmarks are run on more
than eight machines with different amount of NUMA nodes and processor models.
Man has to go through the reports and check the results of all the benchmarks
with different configurations.

When looking at results of benchmark operation, the most important values are
medians from runs with different amount of threads. Unfortunately, due to noise in measurement
and limited amount of repeated runs the medians can be significantly affected by
the noise. This complication brings the need of more detailed inspection of
results than just watching difference in medians of results.

Another useful data are minimum, maximum and quartile values from each
measurement. They reveal the stability of the benchmark and the measurement
noise. With those values it is much easier and more accurate telling the
measurement noise from performance regression. With similar minimum, maximum and
quartile values the performance can be the same even with difference in median values.

The threshold between noise and performance regression is considered as 5\%
difference between base and target measurement but varies by the stability of
each benchmark and complexity of the machine it was run on.

\section{Used technologies}
For classification we use library \emph{scikit-learn} for Python3. Scikit-learn is
a simple library for data analysis and machine learning with BSD open source
license. It is easy to use with fast learning curve and well documented. It is
a good choice for small and medium sized projects that do not need massive
scalability. It provides various algorithms for classification, regression and
clustering built on \emph{NumPy} and \emph{SciPy} Python libraries.

Unlike other machine learning libraries like PyTorch and TensorFlow, the
scikit-learn library does not focus on deep neural networks for larger and
advanced problems. It provides more simple classifiers for easier problems
with smaller datasets where advanced methods would not have enough training
data.

\section{Reading the results}
Learning data come as an XML file containing preprocessed data from
the base and the target run of the benchmark which are labeled as passed or failed.

Record of each benchmark operation is labeled as pass or fail and contains
records of runs with different amount of threads or processes. Those records
contain median, minimum, maximum and quartiles form the repeated runs of the
configuration. The root element contains UUIDs of the comparison and its base
and target results for easier tracing in case of suspicious values.

Here is an example of data for the teaching of classifiers:

\begin{figure}
  \small
  \begin{verbatim}
<comparison_results base_uuid="..." report_uuid="..." target_uuid="...">
  <comparison_result benchmark_name="NASParallel"
      operation_name="bt_C_x" result_status="pass">
    <result median_diff="1" threads_no="4">
      <base_result first_q="7409.2" max="7507.0" mean="7451.7"
        median="7456.5" min="7406.4" stdev="39.2" third_q="7479.3" />
      <target_result first_q="7503.0" max="7597.5" mean="7513.1"
        median="7522.2" min="7404.8" stdev="62.7" third_q="7537.8" />
    </result>
    <result median_diff="0" threads_no="8">
      <base_result first_q="14330.8" max="14740.1" mean="14476.6"
        median="14469.1" min="14325.9" stdev="151.7" third_q="14517.2" />
      <target_result first_q="14494.5" max="14619.3" mean="14481.1"
        median="14521.6" min="14233.7" stdev="130.5" third_q="14536.5" />
    </result>
...
  \end{verbatim}
  \normalsize
  \caption{Part of XML file with labeled data for classification.}
\end{figure}

\section{Labeling of results}
To teach the automatic classifier we need large amount of data for training. To
reduce the time spent on labeling of the passed and failed results we included a
HTML form to report page with comparison of two results to speed up this process.

% TODO talk more about the application
It is a simple server application for committing requests from the form that
runs on a machine used to store the results. The server is written in Python
built on top of the Flask framework. On request it finds given result and modifies
its file with labeling for learning.
% TODO ref to section with XML example

% TODO diagram of labeling process

\begin{figure}
  \centering
  \includegraphics[width=12cm]{teaching_table}
  \caption{Form from HTML comparison report page to label data for machine learning}
\end{figure}

\section{Preprocessing for learning} \label{sec:learn_preprocess}
The intended task is to classify result of each benchmark operation with
different thread configurations which is represented by
\texttt{<comparison\_result>} node in example from previous section. The number
of threads configurations represented by \texttt{threads\_no} attribute in each
\texttt{<result>} is not always the same and varies on the number of CPU cores
on the testing machine.

The important values we focus on are the statistical data from runs with
different amount of threads, that a man uses to label the data manually. From
each of the runs we take minimum, median, maximum and first and third quartile
from the repeated measurements of the same run configuration.

To avoid absolute values from measurements that are different on each machine,
we will use relative proportions. Difference of target and base medians will be
divided by base median and the rest of target statistical values will be divided
by the target median.

Next task is to reduce provided data with variable number of threaded results
to fixed size vector. We will take minimum, median and maximum from values of
each statistical property: of minimums, medians, maximums and first and third
quartiles. This reduction will keep important values form the benchmark
operation results in reasonably small vector.

Last operation with vector values is applying square root on them. This
operation reduces distances of vectors with exceptionally high values, which
helps the k-NN and linear classifier to get better results. Scatter of the
vectors plotted in 3 best dimensions is shown in Figure \ref{fig:vectors_plot}.

For later use, we will name the vector component like \texttt{medians min} is
minimum value of all medians in results with different number of threads.

The whole preprocessing of a single result of benchmark operation shows the
Figure \ref{fig:learn_preprocess} with Python code snippet.

\begin{figure}
  \centering
  \includegraphics[width=14cm]{vectors_plot}
  \caption{In this figure are plotted vectors from the training dataset reduced
    to 3 dimensions, which are the best for classification. The reduction was done by
    \texttt{SelectKBest} class from scikit-learn library by choosing the
    dimensions with the biggest variability between classes. The plot on the
    left shows scatter of vectors from dataset with unmodified values. The plot
    on the right shows the vectors from dataset modified by applying square root
    on their values. This modification and naming of the axis is explained in
    Section \ref{sec:learn_preprocess}}
  \label{fig:vectors_plot}
\end{figure}

\begin{figure}
  \small
  \begin{lstlisting}[language=Python]
mins = []
medians = []
maxes = []
q1s = []
q3s = []

# Iterate over the thread results of single benchmark operation
for res in cpresult:
    for basetarget in res:
        if basetarget.tag == "base_result":
            b_attr = basetarget.attrib
        if basetarget.tag == "target_result":
            t_attr = basetarget.attrib
    try:
        medians.append((float(t_attr["median"]) - float(b_attr["median"]))
                        / float(b_attr["median"]))
        mins.append((float(t_attr["median"]) - float(t_attr["min"]))
                        / float(t_attr["median"]))
        maxes.append((float(t_attr["median"]) - float(t_attr["max"]))
                        / float(t_attr["median"]))
        q1s.append((float(t_attr["median"]) - float(t_attr["first_q"]))
                        / float(t_attr["median"]))
        q3s.append((float(t_attr["median"]) - float(t_attr["third_q"]))
                        / float(t_attr["median"]))
    except:
        print("ERROR in values in " + path)

vector = []
for a in [medians, mins, maxes, q1s, q3s]:
    vector.append(min(a))
    vector.append(median(a))
    vector.append(max(a))

vector = list(map(lambda x: math.sqrt(float(x))
                  if float(x) > 0
                  else -math.sqrt(abs(float(x))), vector)))
  \end{lstlisting}
  \normalsize
  \caption{asdf}
  \label{fig:learn_preprocess}
\end{figure}

\section{Comparison of classifiers}
Classification is procedure of assigning category to new observation based on
training set of observations with specified category. Because of the
availability of already classified data it belongs to \emph{supervised learning}
part of machine learning.

Data for classification are represented by set of vectors with fixed element
count. Vector is an ordered set of numbers where each one represents a value in
separate dimension of the source data. For supervised learning have vectors from
training set assigned categories (also called classes). In our case we will have
2 classes: \emph{pass} and \emph{fail}.

For the following training and evaluation of the models we have roughly 250
labeled vectors. This should be enough to train different simple classifiers with
sufficient accuracy on other unlabeled vectors.

\subsection{Validation and metrics}
cross validation
accuracy

\subsection{k-nearest neighbors classifier}
This classifier does not construct any generalized model, but works with the
whole training data. The decision is made by voting of the $k$ nearest
neighbors.

The learning process consists only of building data structure for more efficient
search through the learning dataset. More computations come with the prediction
when the algorithm must go through the learning dataset and find $k$ nearest
vectors.

Accuracy of the model highly depends on the parameter $k$. Higher values
suppress noise in dataset, but can smoother the boundaries of classes too much.
This behavior can be seen in Figure \ref{fig:knn}.

Scikit-learn implements k-NN classifier in \texttt{KNeighborsClassifier} class.
Next to setting the $k$ number it provides parameter for weighting votes. First
option is uniform voting where each of the $k$ neighbors have the same weight of
the vote. The second option weights votes based on the distance from queried
vector. The weighted option provides better results for our dataset as can be
seen in Figure \ref{fig:knn}.

\begin{figure}
  \centering
  \includegraphics[width=14cm]{knn}
  \caption{Accuracy of k-nearest neighbors classifier on available training
    data with different parameters. On x axis is number of k-nearest neighbors
    that vote for the class of queried vector. The two plotted lines differ in
    weighting of the votes. The first takes all votes with uniform weight and
    the second weights the votes by distance of the neighbors from the queried
    vector.}
  \label{fig:knn}
\end{figure}

\subsection{Linear classifier}
Linear models try to separate vectors of two different classes to two spaces
using a line, plane, hyperplane, etc. depending on the dimensionality of the
vectors.
\ref{fig:vectors_plot}

\subsection{Decision tree classifier}
Decision of this classifier is based on decision tree built on training data.
In each node the tree looks on one component of the queried vector and by
comparison with learned threshold the tree decides to which branch it will
continue. Each leaf of the tree contains label that is returned for the queried
vector which led to the leaf.

Learning of the model involves the creation of the decision tree.

They are easy to interpret and visualize. Example in Figure \ref{fig:decision_tree}.
Disadvantage is high chance of overfitting.

\begin{figure}
  \centering
  \includegraphics[width=14cm]{decision_tree}
  \caption{Part of decision tree created by Decision tree classifier and
    rendered by Graphviz library for Python. It shows the first three layers of the
    decision tree. First line of non-leaf node shows the condition for choosing
    the next decision branch. \emph{gini} property of node shows its impurity --
    proportion of training data not fitting the class of the node. Leaf nodes
    have 0 impurity because they contain vectors of single class. Parameters
    \emph{samples} and \emph{value} show the amount of training vectors for the
    subtree and their division to pass and fail classes.}
  \label{fig:decision_tree}
\end{figure}

\subsection{Forest of randomized trees classifiers}
The forest of randomized trees classifier extends the decision tree
classification by creating a set of decision tree classifiers with small portion
of randomness introduced in the creation of the decision trees. Output of the
classifier is decided by voting of the decision tree classifiers. This
modification compensates the overfitting of single decision tree classifiers.
Although the amount of trees slightly increases the accuracy of the prediction,
the duration of learning of the model and prediction rises linearly with the
amount of trees.

The scikit-learn library provides two implementation of classifiers based on
randomized trees. The first is \texttt{RandomForestClassifier} class. The
dataset for each tree is resampled with replacement creating slightly reduced
and different dataset. When building a decision tree, the component used for
comparison in the node is not the best for splitting, but randomly chosen.

The second is extremely randomized trees method implemented in
\texttt{ExtraTreesClassifier} class. This implementation extends the randomness
by choosing more thresholds for decision randomly and selecting the best instead
of computing the most discriminative one. This method helps to reduce variance
of the model at the cost of higher bias.

Comparison of these two implementations is in Figure
\ref{fig:randomized_forests}. The RandomForestClassifier has slightly better
accuracy than ExtraTreesClassifier. Accuracy also rises much slower compared to
number of the trees in the model.

\begin{figure}
  \centering
  \includegraphics[width=14cm]{randomized_forests}
  \caption{Accuracy of randomized forests classifiers on available training
    dataset with different parameters. On x axis is number of trees that vote
    for the class of queried vector. The first plotted line represents the
    Random forest method and the second the Extra trees method of building the
    trees.}
  \label{fig:randomized_forests}
\end{figure}

\section{Evaluation of classifiers}
What is cross validation
Best classifiers seems to be random forest.
This classifier will be added to analyzer for automatic labeling of comparisons.



\chapter{Future work}
more precise classification with more learning data
classification of results can be included to timelines



\chapter{Conclusion}
In this work we described ...
... was evaluated ...

Závěrečná kapitola obsahuje zhodnocení dosažených výsledků se zvlášť vyznačeným
vlastním přínosem studenta. Povinně se zde objeví i zhodnocení z pohledu dalšího
vývoje projektu, student uvede náměty vycházející ze zkušeností s řešeným
projektem a uvede rovněž návaznosti na právě dokončené projekty (řešené v rámci
ostatních bakalářských prací v daném roce nebo na projekty řešené na externích
pracovištích).


\chapter{Notes}
\begin{verbatim}
zkontrolovat prekladatelem
zminit Red Hat
citovat cely scikit, nebo konkretni stranky
vlozit nedelitelne mezery
stress more what is my work, team and general knowledge
  in our team we use ..., my part of work is ...
klicova slova v lstlisting
jednostranny tisk
rozsirit podekovani

text semestralni prace
licence zdrojaku
lstlisting na jednu stranu
timeliny jako section vizualizace?
(1) a (2) v introductionu

Linux scheduler
    manual pinning option
        numactl
    group imbalance bug
    tuned profiles
        focus on throughput or latency
Performance
    provisioning of machines with Beaker
    collecting system load data
        numastat
        numatop
            usage of cpu and memory on each node
        perf stat
        lstopo for hardware schema image
        free
    scenarios
        variable number of benchmark instances
        with and without pinning processes to specific numa nodes
    result storing
        preprocessing to xml
        storing to database
        aggregating results
        computing statistical data
    plotting
        lstopo for plotting hardware topology
        bargraphs to show inaccuracy
            operations per second (run time of benchmark)
            time out of cpu
        detailed comparison of two results
        timeline with many results over longer period of time
\end{verbatim}

\chapter{Specification}
\begin{enumerate}
\item Get acquainted with the existing methods for measuring performance of the Linux kernel scheduler and with means of storing of benchmarks results for further processing.
\item Study possible ways of processing these results with a focus on graphic interpretation and on methods for detection of performance degradation.
\item Design and implement a method for efficient graphic interpretation of long-term measurements.
\item Design and implement a method for automatic detection of performance regression.
\item Demonstrate the functionality of your implementation on at least two versions of the Linux kernel.
\item Evaluate the obtained results and discuss possibilities of further development of the project, especially of the automatic detection of performance regression.
\end{enumerate}

Literature:
\begin{itemize}
\item Lozi, Jean-Pierre, et al. "The Linux scheduler: a decade of wasted cores." Proceedings of the Eleventh European Conference on Computer Systems. ACM, 2016.
\item Daniel, P., and Cesati Marco. "Understanding the Linux kernel." (2007).
\item Bailey, David H., et al. "The NAS parallel benchmarks." The International Journal of Supercomputing Applications 5.3 (1991): 63-73.
\end{itemize}
